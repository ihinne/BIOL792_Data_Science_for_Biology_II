{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - pandas (YOUR NAME HERE)  \n",
    "### due 3/14/2022   \n",
    "\n",
    "Use this notebook and prompts to complete the homework. Throughout there will be hints and some code provided  \n",
    "\n",
    "### Things you will need:  \n",
    "- Install os, NumPy, pandas  \n",
    "- states_covid.csv  \n",
    "- Bloom_etal_2018_Reduced_Dataset.csv  \n",
    "- logfiles.tgz (or some other multiple file dataset)  \n",
    "\n",
    "*NOTE*: Make sure your PATH is correct  \n",
    "\n",
    "**import packages & check required datasets**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:/Users/amank/Desktop/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas\" #CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"C:/Users/amank/Desktop/Data_Science_For_Biology_II/Part.3.PythonProgramming/Pandas\n"
     ]
    }
   ],
   "source": [
    "#checking my current working directory\n",
    "!cd $PATH\n",
    "!echo \"$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PATH,'states_covid.csv')), 'states_covid.csv does not exist' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(os.path.join(PATH,'Bloom_etal_2018_Reduced_Dataset.csv')), 'Bloom_etal_2018_Reduced_Dataset.csv does not exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - DataFrame manipulation  \n",
    "\n",
    "Using **states_covid.csv**, we are going to read the data in as a DataFrame to manipulate, subset, and filter in various ways.   \n",
    "\n",
    "### Task 1.1 \n",
    "\n",
    "Read in states_covid.csv with date as a \"date\" dtype, and only columns consisting of the hospitalization (4 col), ICU (2 col), and Ventilators (2 col)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>death</th>\n",
       "      <th>deathConfirmed</th>\n",
       "      <th>deathIncrease</th>\n",
       "      <th>deathProbable</th>\n",
       "      <th>hospitalized</th>\n",
       "      <th>hospitalizedCumulative</th>\n",
       "      <th>hospitalizedCurrently</th>\n",
       "      <th>hospitalizedIncrease</th>\n",
       "      <th>...</th>\n",
       "      <th>totalTestResults</th>\n",
       "      <th>totalTestResultsIncrease</th>\n",
       "      <th>totalTestsAntibody</th>\n",
       "      <th>totalTestsAntigen</th>\n",
       "      <th>totalTestsPeopleAntibody</th>\n",
       "      <th>totalTestsPeopleAntigen</th>\n",
       "      <th>totalTestsPeopleViral</th>\n",
       "      <th>totalTestsPeopleViralIncrease</th>\n",
       "      <th>totalTestsViral</th>\n",
       "      <th>totalTestsViralIncrease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>AK</td>\n",
       "      <td>290.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>1653425.0</td>\n",
       "      <td>4640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1653425.0</td>\n",
       "      <td>4640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>AL</td>\n",
       "      <td>9660.0</td>\n",
       "      <td>7575.0</td>\n",
       "      <td>68</td>\n",
       "      <td>2085.0</td>\n",
       "      <td>45250.0</td>\n",
       "      <td>45250.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>2265086.0</td>\n",
       "      <td>4825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115256.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2265086.0</td>\n",
       "      <td>4825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>AR</td>\n",
       "      <td>5377.0</td>\n",
       "      <td>4321.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1056.0</td>\n",
       "      <td>14617.0</td>\n",
       "      <td>14617.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>2609837.0</td>\n",
       "      <td>4779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2609837.0</td>\n",
       "      <td>4779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>AS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>AZ</td>\n",
       "      <td>15650.0</td>\n",
       "      <td>13821.0</td>\n",
       "      <td>148</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>57072.0</td>\n",
       "      <td>57072.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>7478323.0</td>\n",
       "      <td>19439</td>\n",
       "      <td>435091.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3709365.0</td>\n",
       "      <td>6212</td>\n",
       "      <td>7478323.0</td>\n",
       "      <td>19439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date state    death  deathConfirmed  deathIncrease  deathProbable  \\\n",
       "0  2021-02-23    AK    290.0             NaN              0            NaN   \n",
       "1  2021-02-23    AL   9660.0          7575.0             68         2085.0   \n",
       "2  2021-02-23    AR   5377.0          4321.0             14         1056.0   \n",
       "3  2021-02-23    AS      0.0             NaN              0            NaN   \n",
       "4  2021-02-23    AZ  15650.0         13821.0            148         1829.0   \n",
       "\n",
       "   hospitalized  hospitalizedCumulative  hospitalizedCurrently  \\\n",
       "0        1260.0                  1260.0                   38.0   \n",
       "1       45250.0                 45250.0                  762.0   \n",
       "2       14617.0                 14617.0                  545.0   \n",
       "3           NaN                     NaN                    NaN   \n",
       "4       57072.0                 57072.0                 1515.0   \n",
       "\n",
       "   hospitalizedIncrease  ...  totalTestResults  totalTestResultsIncrease  \\\n",
       "0                     9  ...         1653425.0                      4640   \n",
       "1                   122  ...         2265086.0                      4825   \n",
       "2                    47  ...         2609837.0                      4779   \n",
       "3                     0  ...            2140.0                         0   \n",
       "4                    78  ...         7478323.0                     19439   \n",
       "\n",
       "   totalTestsAntibody  totalTestsAntigen  totalTestsPeopleAntibody  \\\n",
       "0                 NaN                NaN                       NaN   \n",
       "1                 NaN                NaN                  115256.0   \n",
       "2                 NaN                NaN                       NaN   \n",
       "3                 NaN                NaN                       NaN   \n",
       "4            435091.0                NaN                       NaN   \n",
       "\n",
       "   totalTestsPeopleAntigen  totalTestsPeopleViral  \\\n",
       "0                      NaN                    NaN   \n",
       "1                      NaN              2265086.0   \n",
       "2                 436309.0                    NaN   \n",
       "3                      NaN                    NaN   \n",
       "4                      NaN              3709365.0   \n",
       "\n",
       "   totalTestsPeopleViralIncrease  totalTestsViral  totalTestsViralIncrease  \n",
       "0                              0        1653425.0                     4640  \n",
       "1                           4825              NaN                        0  \n",
       "2                              0        2609837.0                     4779  \n",
       "3                              0           2140.0                        0  \n",
       "4                           6212        7478323.0                    19439  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_covid_df = pd.read_csv('states_covid.csv') #read in csv\n",
    "state_covid_df.head() #views the top 5 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'state', 'death', 'deathConfirmed', 'deathIncrease',\n",
       "       'deathProbable', 'hospitalized', 'hospitalizedCumulative',\n",
       "       'hospitalizedCurrently', 'hospitalizedIncrease', 'inIcuCumulative',\n",
       "       'inIcuCurrently', 'negative', 'negativeIncrease',\n",
       "       'negativeTestsAntibody', 'negativeTestsPeopleAntibody',\n",
       "       'negativeTestsViral', 'onVentilatorCumulative', 'onVentilatorCurrently',\n",
       "       'positive', 'positiveCasesViral', 'positiveIncrease', 'positiveScore',\n",
       "       'positiveTestsAntibody', 'positiveTestsAntigen',\n",
       "       'positiveTestsPeopleAntibody', 'positiveTestsPeopleAntigen',\n",
       "       'positiveTestsViral', 'recovered', 'totalTestEncountersViral',\n",
       "       'totalTestEncountersViralIncrease', 'totalTestResults',\n",
       "       'totalTestResultsIncrease', 'totalTestsAntibody', 'totalTestsAntigen',\n",
       "       'totalTestsPeopleAntibody', 'totalTestsPeopleAntigen',\n",
       "       'totalTestsPeopleViral', 'totalTestsPeopleViralIncrease',\n",
       "       'totalTestsViral', 'totalTestsViralIncrease'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_covid_df.columns #view the column names and find specified names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hospitalized</th>\n",
       "      <th>hospitalizedCumulative</th>\n",
       "      <th>hospitalizedCurrently</th>\n",
       "      <th>hospitalizedIncrease</th>\n",
       "      <th>inIcuCumulative</th>\n",
       "      <th>inIcuCurrently</th>\n",
       "      <th>onVentilatorCumulative</th>\n",
       "      <th>onVentilatorCurrently</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>45250.0</td>\n",
       "      <td>45250.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>122</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>14617.0</td>\n",
       "      <td>14617.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1505.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-23</td>\n",
       "      <td>57072.0</td>\n",
       "      <td>57072.0</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>447.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>266.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  hospitalized  hospitalizedCumulative  hospitalizedCurrently  \\\n",
       "0 2021-02-23        1260.0                  1260.0                   38.0   \n",
       "1 2021-02-23       45250.0                 45250.0                  762.0   \n",
       "2 2021-02-23       14617.0                 14617.0                  545.0   \n",
       "3 2021-02-23           NaN                     NaN                    NaN   \n",
       "4 2021-02-23       57072.0                 57072.0                 1515.0   \n",
       "\n",
       "   hospitalizedIncrease  inIcuCumulative  inIcuCurrently  \\\n",
       "0                     9              NaN             NaN   \n",
       "1                   122           2632.0             NaN   \n",
       "2                    47              NaN           204.0   \n",
       "3                     0              NaN             NaN   \n",
       "4                    78              NaN           447.0   \n",
       "\n",
       "   onVentilatorCumulative  onVentilatorCurrently  \n",
       "0                     NaN                    5.0  \n",
       "1                  1497.0                    NaN  \n",
       "2                  1505.0                   99.0  \n",
       "3                     NaN                    NaN  \n",
       "4                     NaN                  266.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_covid_sub_df = pd.read_csv('states_covid.csv',usecols=['date','hospitalized', 'hospitalizedCumulative',\n",
    "       'hospitalizedCurrently', 'hospitalizedIncrease', 'inIcuCumulative','inIcuCurrently', 'onVentilatorCumulative', 'onVentilatorCurrently'],parse_dates=['date'],infer_datetime_format=True)\n",
    "state_covid_sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date                      datetime64[ns]\n",
       "hospitalized                     float64\n",
       "hospitalizedCumulative           float64\n",
       "hospitalizedCurrently            float64\n",
       "hospitalizedIncrease               int64\n",
       "inIcuCumulative                  float64\n",
       "inIcuCurrently                   float64\n",
       "onVentilatorCumulative           float64\n",
       "onVentilatorCurrently            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_covid_sub_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 \n",
    "\n",
    "For each of the following catergories: *currently* hospitalized, *currently* in the ICU, and *currently* on ventilation...  \n",
    "Find the 5 states with the greatest numbers in each catergory and list them in order.      \n",
    "*hint*: sort_values, unique  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 States by Current Hospitalizations:\n",
      "        hospitalizedCurrently\n",
      "state                       \n",
      "CA                   22851.0\n",
      "NY                   18825.0\n",
      "TX                   14218.0\n",
      "FL                    9520.0\n",
      "NJ                    8270.0\n",
      "\n",
      "Top 5 States by Current ICU Patients:\n",
      "        inIcuCurrently\n",
      "state                \n",
      "NY             5225.0\n",
      "CA             4971.0\n",
      "TX             3686.0\n",
      "NJ             2051.0\n",
      "MI             1663.0\n",
      "\n",
      "Top 5 States by Current Ventilator Patients:\n",
      "        onVentilatorCurrently\n",
      "state                       \n",
      "NY                    2425.0\n",
      "NJ                    1705.0\n",
      "MI                    1441.0\n",
      "OH                     863.0\n",
      "AZ                     821.0\n"
     ]
    }
   ],
   "source": [
    "# Find the top 5 states with the greatest number of patients currently hospitalized\n",
    "top_5_hosp = state_covid_df[['state', 'hospitalizedCurrently']].groupby(['state']).max().sort_values(by='hospitalizedCurrently', ascending=False).head()\n",
    "print(\"Top 5 States by Current Hospitalizations:\\n\", top_5_hosp)\n",
    "\n",
    "# Find the top 5 states with the greatest number of patients currently in the ICU\n",
    "top_5_icu = state_covid_df[['state', 'inIcuCurrently']].groupby(['state']).max().sort_values(by='inIcuCurrently', ascending=False).head()\n",
    "print(\"\\nTop 5 States by Current ICU Patients:\\n\", top_5_icu)\n",
    "\n",
    "# Find the top 5 states with the greatest number of patients currently on ventilators\n",
    "top_5_vent = state_covid_df[['state', 'onVentilatorCurrently']].groupby(['state']).max().sort_values(by='onVentilatorCurrently', ascending=False).head()\n",
    "print(\"\\nTop 5 States by Current Ventilator Patients:\\n\", top_5_vent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 States by hospitalizedCurrently:\n",
      "       hospitalizedCurrently\n",
      "state                       \n",
      "CA                   22851.0\n",
      "NY                   18825.0\n",
      "TX                   14218.0\n",
      "FL                    9520.0\n",
      "NJ                    8270.0\n",
      "\n",
      "Top 5 States by inIcuCurrently:\n",
      "       inIcuCurrently\n",
      "state                \n",
      "NY             5225.0\n",
      "CA             4971.0\n",
      "TX             3686.0\n",
      "NJ             2051.0\n",
      "MI             1663.0\n",
      "\n",
      "Top 5 States by onVentilatorCurrently:\n",
      "       onVentilatorCurrently\n",
      "state                       \n",
      "NY                    2425.0\n",
      "NJ                    1705.0\n",
      "MI                    1441.0\n",
      "OH                     863.0\n",
      "AZ                     821.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the categories of interest\n",
    "categories = ['hospitalizedCurrently', 'inIcuCurrently', 'onVentilatorCurrently']\n",
    "\n",
    "# Iterate through the categories and print the top 5 states for each category\n",
    "for cat in categories:\n",
    "    # Group the data by state and find the maximum value for each state in the current category\n",
    "    max_values = state_covid_df[['state', cat]].groupby(['state']).max().sort_values(by=cat, ascending=False)\n",
    "    \n",
    "    # Print the top 5 states for the current category\n",
    "    print(f\"Top 5 States by {cat}:\\n{max_values.head()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3 \n",
    "\n",
    "Find the date in which each state crossed 1000 cumulative hospitilized covid patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state\n",
      "AK    2021-02-23\n",
      "AL    2021-02-23\n",
      "AR    2021-02-23\n",
      "AZ    2021-02-23\n",
      "CO    2021-02-23\n",
      "CT    2021-02-23\n",
      "FL    2021-02-23\n",
      "GA    2021-02-23\n",
      "HI    2021-02-23\n",
      "ID    2021-02-23\n",
      "IN    2021-02-23\n",
      "KS    2021-02-23\n",
      "KY    2021-02-23\n",
      "MA    2021-02-23\n",
      "MD    2021-02-23\n",
      "ME    2021-02-23\n",
      "MN    2021-02-23\n",
      "MS    2021-02-23\n",
      "MT    2021-02-23\n",
      "ND    2021-02-23\n",
      "NE    2021-02-23\n",
      "NH    2021-02-23\n",
      "NJ    2021-02-23\n",
      "NM    2021-02-23\n",
      "NY    2021-02-23\n",
      "OH    2021-02-23\n",
      "OK    2021-02-23\n",
      "OR    2021-02-23\n",
      "PA    2020-04-06\n",
      "RI    2021-02-23\n",
      "SC    2021-02-23\n",
      "SD    2021-02-23\n",
      "TN    2021-02-23\n",
      "UT    2021-02-23\n",
      "VA    2021-02-23\n",
      "WA    2021-02-23\n",
      "WI    2021-02-23\n",
      "WY    2021-02-23\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Filter the data for only the columns of interest\n",
    "state_covid_df_hosp = state_covid_df[['date', 'state', 'hospitalizedCumulative']]\n",
    "\n",
    "# Group the data by state and find the first date in which each state crossed 1000 hospitalized patients\n",
    "date_1000 = state_covid_df_hosp[df_hosp['hospitalizedCumulative'] >= 1000].groupby('state')['date'].first()\n",
    "\n",
    "# Print the result\n",
    "print(date_1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK: 2021-02-23\n",
      "AL: 2021-02-23\n",
      "AR: 2021-02-23\n",
      "AZ: 2021-02-23\n",
      "CO: 2021-02-23\n",
      "CT: 2021-02-23\n",
      "FL: 2021-02-23\n",
      "GA: 2021-02-23\n",
      "HI: 2021-02-23\n",
      "ID: 2021-02-23\n",
      "IN: 2021-02-23\n",
      "KS: 2021-02-23\n",
      "KY: 2021-02-23\n",
      "MA: 2021-02-23\n",
      "MD: 2021-02-23\n",
      "ME: 2021-02-23\n",
      "MN: 2021-02-23\n",
      "MS: 2021-02-23\n",
      "MT: 2021-02-23\n",
      "ND: 2021-02-23\n",
      "NE: 2021-02-23\n",
      "NH: 2021-02-23\n",
      "NJ: 2021-02-23\n",
      "NM: 2021-02-23\n",
      "NY: 2021-02-23\n",
      "OH: 2021-02-23\n",
      "OK: 2021-02-23\n",
      "OR: 2021-02-23\n",
      "PA: 2020-04-06\n",
      "RI: 2021-02-23\n",
      "SC: 2021-02-23\n",
      "SD: 2021-02-23\n",
      "TN: 2021-02-23\n",
      "UT: 2021-02-23\n",
      "VA: 2021-02-23\n",
      "WA: 2021-02-23\n",
      "WI: 2021-02-23\n",
      "WY: 2021-02-23\n"
     ]
    }
   ],
   "source": [
    "# Using a for loop\n",
    "\n",
    "# Filter the data for only the columns of interest\n",
    "state_covid_df_hosp = state_covid_df[['date', 'state', 'hospitalizedCumulative']]\n",
    "\n",
    "# Create an empty dictionary to store the result\n",
    "date_1000 = {}\n",
    "\n",
    "# Iterate through each state abbreviation and find the first date in which it crossed 1000 hospitalized patients\n",
    "for state in state_covid_df_hosp['state'].unique():\n",
    "    state_data = state_covid_df_hosp[state_covid_df_hosp['state'] == state]\n",
    "    if len(state_data) > 0 and (state_data['hospitalizedCumulative'] >= 1000).any():\n",
    "        date = state_data[state_data['hospitalizedCumulative'] >= 1000]['date'].iloc[0]\n",
    "        date_1000[state] = date\n",
    "        \n",
    "# Print the result\n",
    "#print(date_1000)\n",
    "\n",
    "# Print the result\n",
    "for state, date in date_1000.items():\n",
    "   print(f\"{state}: {date}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - DataFrame summarizing  \n",
    "\n",
    "Using **Bloom_etal_2018_Reduced_Dataset.csv**, we are going to do more dataframe manipulation and subsetting and summarizing    \n",
    "\n",
    "### Task 2.1 \n",
    "\n",
    "Read in Bloom_etal_2018_Reduced_Dataset.csv and create two new columns ('genus','species') that consists of the column *taxa* split at the underscore. Print out the head of this new dataframe and the number of unique genera**     \n",
    "\n",
    "*hint:* pd.str.split(,expand=True)  \n",
    "\n",
    "for example:  \n",
    "\n",
    "| taxa | genus | species   \n",
    "| :------ | :-- | :---   \n",
    "| Alosa_alabamae | Alosa | alabamae  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   taxa  logbodysize  trophic_position         Reg  genus  \\\n",
      "0        Alosa_alabamae     1.707570          0.431364  diadromous  Alosa   \n",
      "1           Alosa_alosa     1.778151          0.556303  diadromous  Alosa   \n",
      "2          Alosa_fallax     1.778151          0.556303  diadromous  Alosa   \n",
      "3       Alosa_mediocris     1.778151          0.612784  diadromous  Alosa   \n",
      "4  Alosa_pseudoharengus     1.602060          0.544068  diadromous  Alosa   \n",
      "\n",
      "          species  \n",
      "0        alabamae  \n",
      "1           alosa  \n",
      "2          fallax  \n",
      "3       mediocris  \n",
      "4  pseudoharengus  \n",
      "Number of unique genera: 34\n"
     ]
    }
   ],
   "source": [
    "# Read in the CSV file\n",
    "bloom_df = pd.read_csv('Bloom_etal_2018_Reduced_Dataset.csv')\n",
    "\n",
    "# Split the 'taxa' column into two new columns: 'genus' and 'species'\n",
    "bloom_df[['genus', 'species']] = bloom_df['taxa'].str.split('_', n=1, expand=True)\n",
    "\n",
    "# Print out the head of the new dataframe with the taxa, genus, and species columns\n",
    "print(bloom_df.head())\n",
    "\n",
    "# Print out the number of unique genera\n",
    "unique_genera = bloom_df['genus'].nunique()\n",
    "print(\"Number of unique genera: \", unique_genera)\n",
    "\n",
    "or\n",
    "#print(\"Number of unique genera:\", bloom_df['genus'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 \n",
    "\n",
    "Create a new dataframe with the mean *logbodysize* and *trophicposition* of each genera. Sort this data frame by the largest body size. Print the head of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            logbodysize  trophic_position\n",
      "genus                                    \n",
      "Tenualosa      1.778151          0.462398\n",
      "Alosa          1.739062          0.540815\n",
      "Coilia         1.544068          0.477121\n",
      "Ethmalosa      1.544068          0.397940\n",
      "Potamalosa     1.505150          0.518514\n"
     ]
    }
   ],
   "source": [
    "# Group the data by genera and calculate the mean logbodysize and trophicposition for each group\n",
    "genera_data = bloom_df.groupby(['genus']).mean()[['logbodysize', 'trophic_position']]\n",
    "\n",
    "# Sort the genera_data dataframe by body size in descending order\n",
    "genera_data = genera_data.sort_values(by='logbodysize', ascending=False)\n",
    "\n",
    "# Print the head of the genera_data dataframe\n",
    "print(genera_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which genera is the smallest and largest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest genus: Amazonsprattus\n",
      "Largest genus: Tenualosa\n"
     ]
    }
   ],
   "source": [
    "# Find the smallest and largest genera\n",
    "smallest_genus = genera_data.loc[genera_data['logbodysize'].idxmin()]\n",
    "largest_genus = genera_data.loc[genera_data['logbodysize'].idxmax()]\n",
    "\n",
    "# Print the names of the smallest and largest genera\n",
    "print(\"Smallest genus:\", smallest_genus.name)\n",
    "print(\"Largest genus:\", largest_genus.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest genus: Amazonsprattus\n",
      "Largest genus: Tenualosa\n"
     ]
    }
   ],
   "source": [
    "# Find the genus with the smallest mean logbodysize\n",
    "smallest_genus = genera_data['logbodysize'].idxmin()\n",
    "print(\"Smallest genus:\", smallest_genus)\n",
    "\n",
    "# Find the genus with the largest mean logbodysize\n",
    "largest_genus = genera_data['logbodysize'].idxmax()\n",
    "print(\"Largest genus:\", largest_genus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the trophic position of the smallest and largest?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest genus: Amazonsprattus\n",
      "Smallest genus trophic position: 0.531478917\n",
      "Largest genus: Tenualosa\n",
      "Largest genus trophic position: 0.462397998\n"
     ]
    }
   ],
   "source": [
    "# Find the genus with the smallest mean logbodysize and its corresponding trophic position\n",
    "smallest_genus = genera_data['logbodysize'].idxmin()\n",
    "smallest_trophic_position = genera_data.loc[smallest_genus]['trophic_position']\n",
    "print(\"Smallest genus:\", smallest_genus)\n",
    "print(\"Smallest genus trophic position:\", smallest_trophic_position)\n",
    "\n",
    "# Find the genus with the largest mean logbodysize and its corresponding trophic position\n",
    "largest_genus = genera_data['logbodysize'].idxmax()\n",
    "largest_trophic_position = genera_data.loc[largest_genus]['trophic_position']\n",
    "print(\"Largest genus:\", largest_genus)\n",
    "print(\"Largest genus trophic position:\", largest_trophic_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Read in muliple files to a dictionary and make a DataFrame - **OPTIONAL/BONUS**  \n",
    "\n",
    "### This is not something you are expected to do in this course, but just here to give you an idea of the things that you COULD do. Answers will be posted after due date.  \n",
    "\n",
    "\n",
    "Using **logfiles**: we are going to do read in each file, get some data, append it to a dictionary to later make into a dataframe.     \n",
    "\n",
    "**note:** *make sure to unzip logfiles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amank\\Desktop\\Data_Science_For_Biology_II\\Part.3.PythonProgramming\\Pandas\n"
     ]
    }
   ],
   "source": [
    "cd $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: #unzip: Not found in archive\n",
      "tar: Error exit delayed from previous errors.\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf logfiles.tgz #unzip logfiles \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadError",
     "evalue": "not a gzip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadGzipFile\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1691\u001b[0m, in \u001b[0;36mTarFile.gzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1691\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mtaropen(name, mode, fileobj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1668\u001b[0m, in \u001b[0;36mTarFile.taropen\u001b[1;34m(cls, name, mode, fileobj, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(name, mode, fileobj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1531\u001b[0m, in \u001b[0;36mTarFile.__init__\u001b[1;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirstmember \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirstmember \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1534\u001b[0m     \u001b[38;5;66;03m# Move to the end of the archive,\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m     \u001b[38;5;66;03m# before the first empty block.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:2363\u001b[0m, in \u001b[0;36mTarFile.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2364\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:2336\u001b[0m, in \u001b[0;36mTarFile.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2336\u001b[0m     tarinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarinfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromtarfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EOFHeaderError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1121\u001b[0m, in \u001b[0;36mTarInfo.fromtarfile\u001b[1;34m(cls, tarfile)\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;124;03m\"\"\"Return the next TarInfo object from TarFile object\u001b[39;00m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;124;03m   tarfile.\u001b[39;00m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1121\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBLOCKSIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrombuf(buf, tarfile\u001b[38;5;241m.\u001b[39mencoding, tarfile\u001b[38;5;241m.\u001b[39merrors)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\gzip.py:300\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\gzip.py:487\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_read()\n\u001b[1;32m--> 487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\gzip.py:435\u001b[0m, in \u001b[0;36m_GzipReader._read_gzip_header\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\037\u001b[39;00m\u001b[38;5;130;01m\\213\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadGzipFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot a gzipped file (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m magic)\n\u001b[0;32m    437\u001b[0m (method, flag,\n\u001b[0;32m    438\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_mtime) \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BBIxx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_exact(\u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[1;31mBadGzipFile\u001b[0m: Not a gzipped file (b'._')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [116]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogfiles.tgz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr:gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m tar:\n\u001b[0;32m      4\u001b[0m     tar\u001b[38;5;241m.\u001b[39mextractall()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1638\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown compression type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m comptype)\n\u001b[1;32m-> 1638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(name, filemode, fileobj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1641\u001b[0m     filemode, comptype \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\tarfile.py:1695\u001b[0m, in \u001b[0;36mTarFile.gzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     fileobj\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m-> 1695\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot a gzip file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1696\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[1;31mReadError\u001b[0m: not a gzip file"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('logfiles.tgz', 'r:gz') as tar:\n",
    "    tar.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(PATH,'logfiles')\n",
    "assert os.path.exists(log_dir), 'log_dir does not exist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to find the necessary files. The number of files in the log files is 36, make sure you have that many as well  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls -l logfiles/*txt | wc -l  #works in bash to get 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "!dir logfiles\\*txt /b |find /v /c \"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "text_files = [f for f in os.listdir(log_dir) if f.endswith('.txt')]\n",
    "print(len(text_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [118]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m logfiles \u001b[38;5;241m=\u001b[39m get_ipython()\u001b[38;5;241m.\u001b[39mgetoutput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind $log_dir -name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*txt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m #unix command to find files in log_dir directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m logfiles \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m logfiles] \u001b[38;5;66;03m#this finds the full path to the file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlogfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "logfiles = !find $log_dir -name '*txt' #unix command to find files in log_dir directory\n",
    "logfiles = [os.path.abspath(x) for x in logfiles] #this finds the full path to the file\n",
    "print(logfiles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Do not have correct number of logfiles",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [119]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(logfiles)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m36\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDo not have correct number of logfiles\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Do not have correct number of logfiles"
     ]
    }
   ],
   "source": [
    "assert(len(logfiles)==36), 'Do not have correct number of logfiles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a little tricky here  \n",
    "\n",
    "Read in each of the logfiles, for each file extract:  \n",
    "- minimum temperature  \n",
    "- maximum temperature  \n",
    "- date of minimum temp   \n",
    "- date of maximum temp   \n",
    "- mean temp for each file.   \n",
    "\n",
    "This data should all be appended for a dictionary within a for loop:    \n",
    "Key should be the file name without the path or .txt extension  \n",
    "Values should be (minTemp,maxTemp,minDate,maxDate,meanTemp)\n",
    "\n",
    "I recommend making this work for one file first, then putting the rest in a for loop to do the rest.  \n",
    "\n",
    "Below is an example of how to read in one file\n",
    "\n",
    "*hint:* do not read date in as date object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>8:00:00 AM</td>\n",
       "      <td>47.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>8:35:00 AM</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>9:10:00 AM</td>\n",
       "      <td>48.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>9:45:00 AM</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9/16/2013</td>\n",
       "      <td>10:20:00 AM</td>\n",
       "      <td>50.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date         Time  Temp\n",
       "0  9/16/2013   8:00:00 AM  47.9\n",
       "1  9/16/2013   8:35:00 AM  48.2\n",
       "2  9/16/2013   9:10:00 AM  48.7\n",
       "3  9/16/2013   9:45:00 AM  49.4\n",
       "4  9/16/2013  10:20:00 AM  50.3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### set up data frame as you read in each file\n",
    "infile = pd.read_csv(logfiles[0],sep='\\t',engine='python')\n",
    "infile.columns = ['Index','Date','Time','Temp','Type']\n",
    "infile = infile[['Date','Time','Temp']]\n",
    "infile.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do everything in steps, make sure it works. Calculate summaries with this one infile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-26-d888355c2242>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-d888355c2242>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    minTemp =\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "minTemp =   \n",
    "maxTemp =  \n",
    "minDate = infile['Date'][infile['Temp'] == infile['Temp'].min()].unique()[0] #use this for minDate \n",
    "maxDate = infile['Date'][infile['Temp'] == infile['Temp'].max()].unique()[0] #use this for maxDate \n",
    "meanTemp = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you started, I suggest writing some dummy code in plain words to help outline your for loop:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfiles_dict = {}  \n",
    "for f in logfiles:  \n",
    "    #do something   \n",
    "    #do not read date in as date object  \n",
    "    #do more something   \n",
    "    #do other stuff  \n",
    "    #make print statements EVERYWHERE  \n",
    "    #append to dict  \n",
    "    #blahbahblah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do the real code below here. You don't need to turnin your thoughts. Just put it in there as a help reminder. Most people all still do this, no matter how advanced they are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once you have created a DataFrame with all the logfiles, print the head and save it to an outfile using pd.to_csv() as logfiles_df.csv** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is an example of the final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minTemp</th>\n",
       "      <th>maxTemp</th>\n",
       "      <th>minDate</th>\n",
       "      <th>maxDate</th>\n",
       "      <th>meanTemp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1901302235_H6_S_14</th>\n",
       "      <td>21.3</td>\n",
       "      <td>65.8</td>\n",
       "      <td>12/6/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>35.726191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302121_H8_S_14</th>\n",
       "      <td>17.9</td>\n",
       "      <td>73.4</td>\n",
       "      <td>10/28/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>34.698866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302217_H18_D_14</th>\n",
       "      <td>21.0</td>\n",
       "      <td>82.4</td>\n",
       "      <td>10/28/2013</td>\n",
       "      <td>7/26/2014</td>\n",
       "      <td>33.334564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302110_H13_S_14</th>\n",
       "      <td>22.8</td>\n",
       "      <td>60.9</td>\n",
       "      <td>10/4/2013</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>32.240032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901302146_H14_D_14</th>\n",
       "      <td>24.6</td>\n",
       "      <td>66.1</td>\n",
       "      <td>1/21/2014</td>\n",
       "      <td>7/13/2014</td>\n",
       "      <td>34.242075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     minTemp  maxTemp     minDate    maxDate   meanTemp\n",
       "1901302235_H6_S_14      21.3     65.8   12/6/2013  7/13/2014  35.726191\n",
       "1901302121_H8_S_14      17.9     73.4  10/28/2013  7/13/2014  34.698866\n",
       "1901302217_H18_D_14     21.0     82.4  10/28/2013  7/26/2014  33.334564\n",
       "1901302110_H13_S_14     22.8     60.9   10/4/2013  7/13/2014  32.240032\n",
       "1901302146_H14_D_14     24.6     66.1   1/21/2014  7/13/2014  34.242075"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logfiles_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
